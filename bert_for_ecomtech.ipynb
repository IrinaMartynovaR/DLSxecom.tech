{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FPP8aoN8_cW3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "from torch.optim import AdamW  \n",
    "from transformers import get_linear_schedule_with_warmup, Trainer, TrainingArguments\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value = 24\n",
    "# set_seed(seed_value)\n",
    "# torch.manual_seed(seed_value)\n",
    "# torch.cuda.manual_seed_all(seed_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## чистим теги и переводим на русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(tags):\n",
    "    cleaned = re.sub(r'[\\{\\}]', '', tags)\n",
    "    cleaned = cleaned.replace(\" \", \"\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_tags\"] = df[\"tags\"].astype(str).apply(clean_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROMOTIONS', 'PRODUCTS_QUALITY', 'nan', 'ASSORTMENT', 'SUPPORT', 'PRICE', 'DELIVERY', 'CATALOG_NAVIGATION', 'PAYMENT'}\n"
     ]
    }
   ],
   "source": [
    "all_tags = ','.join(df[\"cleaned_tags\"].dropna())\n",
    "words = all_tags.split(',')\n",
    "unique_words = set(words)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tags = {\n",
    "    'SUPPORT': 'поддержка',\n",
    "    'DELIVERY': 'доставка',\n",
    "    'PROMOTIONS': 'акции',\n",
    "    'CATALOG_NAVIGATION': 'навигация по каталогу',\n",
    "    'PRODUCTS_QUALITY': 'качество продукции',\n",
    "    'PRICE': 'цена',\n",
    "    'PAYMENT': 'оплата',\n",
    "    'ASSORTMENT': 'ассортимент'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tags(tags):\n",
    "    tag_list = tags.split(',')\n",
    "    translated_tags = [dict_tags.get(tag, tag) for tag in tag_list]\n",
    "    return ','.join(translated_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assessment ухудшил скор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tags_ru'] = df['cleaned_tags'].apply(translate_tags)\n",
    "df['text_tags'] = df['text'] + '.'+ df['cleaned_tags_ru'] + ' '+ df['tags'].astype(str)# + ' '+ df['assessment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Отличное приложение спасибо большое за доставку благодарю вас.цена,акции,оплата,доставка,поддержка,ассортимент,навигация по каталогу {PRICE,PROMOTIONS,PAYMENT,DELIVERY,SUPPORT,ASSORTMENT,CATALOG_NAVIGATION}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tags'][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## длина отзывов и количество уникальных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1733.,  914.,  608.,  396.,  266.,  207.,  222.,  207.,   64.,\n",
       "           6.]),\n",
       " array([ 1. ,  6.1, 11.2, 16.3, 21.4, 26.5, 31.6, 36.7, 41.8, 46.9, 52. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"text\"].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество: 8629\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "unique_tokens = set()\n",
    "df['text'].apply(lambda x: unique_tokens.update(tokenizer.tokenize(x)))\n",
    "num_unique_tokens = len(unique_tokens)\n",
    "print(f\"Количество: {num_unique_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG1EvQjeOGd0",
    "outputId": "cf641520-7724-4896-cd95-734411d44d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В тренировочную выборку попало: 3698\n",
      "В валидационную выборку попало: 925\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.2\n",
    "train_df, test_df = train_test_split(df, test_size=test_split)\n",
    "\n",
    "print(f\"В тренировочную выборку попало: {len(train_df)}\")\n",
    "print(f\"В валидационную выборку попало: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## отбираем лейблы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O2oUMBL7HJAX"
   },
   "outputs": [],
   "source": [
    "not_chosen_columns = ['assessment', 'text','tags', 'text_tags', 'cleaned_tags_ru', 'cleaned_tags']\n",
    "label_columns = [col for col in df.columns if col not in not_chosen_columns]\n",
    "\n",
    "df_labels_train = train_df[label_columns]\n",
    "df_labels_test = test_df[label_columns]\n",
    "labels_list_train = df_labels_train.values.tolist()\n",
    "labels_list_test = df_labels_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WzzIS15SIuq5"
   },
   "outputs": [],
   "source": [
    "labels_list_train = [[float(label) for label in labels] for labels in labels_list_train]\n",
    "labels_list_test = [[float(label) for label in labels] for labels in labels_list_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJO2KkV8nvTv"
   },
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jhzl4solDjok"
   },
   "outputs": [],
   "source": [
    "train_texts = train_df['text_tags'].fillna('').astype(str).tolist()\n",
    "train_labels = labels_list_train\n",
    "\n",
    "eval_texts = test_df['text_tags'].fillna('').astype(str).tolist()\n",
    "eval_labels = labels_list_test\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2', do_lower_case=True)# павлов отработал хуже\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "eval_encodings = tokenizer(eval_texts, padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassData(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextClassData(train_encodings, train_labels)\n",
    "eval_dataset = TextClassData(eval_encodings, eval_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model.bert.encoder.layer[0:2] не сильно влияет, улучшает только скорость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Gik7DrEpvFjy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17400' max='17400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17400/17400 58:37, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.379435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.160689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.128680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.122706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.119060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.116696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.115096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.113967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.113171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.112572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.112143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.111821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.111565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.111353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.111206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.111073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.110963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.110882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.110799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.110403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.109733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.108595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.107231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.105928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.104682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.103324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.101963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.100794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.099407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.096836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.095513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.094073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.092721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.091702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.090025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.088655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.087542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.086252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.085147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.084122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.082744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.081821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.080592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.079758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.078709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.077697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.076919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.076034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.075165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.074635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.073693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.073063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.072464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.071907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.071385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.070871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.070293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.069821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.069580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.069246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.068868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.068330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.068065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.067670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.067872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.067407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.066814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.066529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.066410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.066266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.066010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.065740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.065630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.065492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.065333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.065175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.064937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.065055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.065140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.064781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.064567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.064609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.064223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.064166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.064214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.064042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.064004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.063806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.064010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.063780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.063701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.063724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.063767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.063739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.063635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.063690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.063533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.063449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.063321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.063374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.063405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.063414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.063309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.063207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.063294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.063303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.063484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.063329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.063330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.063277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.063127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.063266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.063224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.063273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.063202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.063227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.063169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.063225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.063149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.063201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.063135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.063244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.063150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.063153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.063126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.063161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.063088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.063150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.063126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.063174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.063071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.063121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.063130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.063101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.063084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.063067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.063068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.063111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.063101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.063096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.063090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.063096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.063094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.063094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17400, training_loss=0.06725109659392259, metrics={'train_runtime': 3517.6738, 'train_samples_per_second': 157.689, 'train_steps_per_second': 4.946, 'total_flos': 4116074082508800.0, 'train_loss': 0.06725109659392259, 'epoch': 150.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 2e-5 \n",
    "weight_decay = 0.1  \n",
    "train_epochs = 150 #будем специально переобучаться\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=train_epochs,\n",
    ")\n",
    "\n",
    "\n",
    "num_training_steps = len(train_dataset) // training_arguments.per_device_train_batch_size * training_arguments.num_train_epochs\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,  \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    optimizers=(optimizer, scheduler), \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_sub = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_sub[\"cleaned_tags\"] = texts_sub[\"tags\"].astype(str).apply(clean_tags)\n",
    "texts_sub['cleaned_tags_ru'] = texts_sub['cleaned_tags'].apply(translate_tags)\n",
    "texts_sub['text_tags'] = texts_sub['text'] + ' '+ texts_sub['cleaned_tags_ru'] + ' '+ texts_sub['tags'].astype(str)# + ' '+ texts_sub['assessment'].astype(str)\n",
    "# texts_sub['text_tags'] = texts_sub['text'] + ' '+ texts_sub['cleaned_tags_ru']\n",
    "# texts_sub['text_tags'] = texts_sub['text'] + ' '+ texts_sub['tags']\n",
    "sub_texts = texts_sub['text_tags'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ihUmtx99GgaK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fn8NQu_0qER3"
   },
   "outputs": [],
   "source": [
    "text_list = sub_texts\n",
    "all_probabilities = []\n",
    "\n",
    "\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    all_probabilities.append(probabilities)\n",
    "    \n",
    "    \n",
    "all_probabilities_array = np.array([tensor.cpu().numpy() for tensor in all_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = 0.5 #0.5 150 2e5 0.1 = 46 <-лучший скор\n",
    "df_submission = pd.DataFrame(columns=label_columns)\n",
    "\n",
    "\n",
    "for row in all_probabilities_array:\n",
    "    mask = row >= new_threshold\n",
    "    if mask.any():\n",
    "        thresholded_row = mask.astype(int)\n",
    "    else:\n",
    "        top_1_index = np.argmax(row)\n",
    "        thresholded_row = np.zeros(len(label_columns), dtype=int)\n",
    "        thresholded_row[top_1_index] = 1\n",
    "    thresholded_row = thresholded_row.reshape(1, -1)\n",
    "    df_submission = pd.concat([df_submission, pd.DataFrame(thresholded_row, columns=label_columns)], ignore_index=True)\n",
    "    \n",
    "    \n",
    "df_submission = df_submission.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['index'] = texts_sub['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [re.sub(r'\\D', '', col) for col in df_submission.columns[:-1]]  \n",
    "new_columns.append('index') \n",
    "\n",
    "if len(new_columns) == len(df_submission.columns):\n",
    "    df_submission.columns = new_columns\n",
    "\n",
    "binary_columns = df_submission.columns[:-1]\n",
    "\n",
    "def create_target_column(row):\n",
    "    cols_with_ones = [col for col in binary_columns if row[col] == 1]\n",
    "    return ' '.join(cols_with_ones)\n",
    "\n",
    "df_submission['target'] = df_submission.apply(create_target_column, axis=1)\n",
    "submission_res = df_submission[['index', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3135</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4655</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22118</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>3523</td>\n",
       "      <td>3 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>24925</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>6327</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9013</th>\n",
       "      <td>530</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9014</th>\n",
       "      <td>20221</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index target\n",
       "0      3135    1 2\n",
       "1      4655     12\n",
       "2     22118    1 2\n",
       "3     23511      0\n",
       "4        45     18\n",
       "...     ...    ...\n",
       "9010   3523   3 30\n",
       "9011  24925     28\n",
       "9012   6327      8\n",
       "9013    530     15\n",
       "9014  20221      8\n",
       "\n",
       "[9015 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_res.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
